<!DOCTYPE html>
<html>
  <head>
    <title>From Hot Mess to Information</title>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="style.css">
  </head>
  <body>
    <textarea id="source">

class: center, middle
counter: false

# From hot mess to Information
.example[
### or why you should spend more time processing your data
]
<br><br><br>

### Serena Peruzzo 

Senior Data Scientist @ Bardess Group

---

class: middle, center

# Data Science Workflow

![](img/ds-workflow.png)

.footnote[.right[_https://houseofbots.com/news-detail/3320-Streamlining-the-Data-Scientists-Workflow_]]

<!-- ---

class: middle, center

.red[
# Warning
]

## This talk is not about fancy algorithms -->

---
class: center, middle

![](img/piechart.png)
.footnote[.right[[2016 Data Science Report](https://visit.figure-eight.com/data-science-report.html) ![](img/crowdflower_logo.png)]]

---
class: center, middle

![](img/headline2.png)

![](img/headline1.png)

![](img/headline3.png)

<!-- ---

class: middle, center

.left[![](img/quotation-left.png)] 80 percent of a data scientistâ€™s valuable time is spent simply finding, cleansing, and organizing data, leaving only 20 percent to actually perform analysis .right[![](img/quotation-right.png)]
.right[IBM Data Analytics] -->

---

class: middle

# Two Missions

### 1. None of this is easy or trivial

### 2. Adds value to the project

<!-- ### 3. Some of the stats used is actually pretty interesting -->

---

class: top

# Garbage in, Garbage out
* Machine Learning models are not smart per se

* Simple algorithms can outperform more complex ones if trained on enough high quality data

* Incorrect, inconsistent, biased data leads to wrong conclusions

* The implications are serious

.right[![](img/gigo.png)]

---

class: middle

# Agenda

* ### From messy data to tidy data

* ### From tidy data to clean data:
  * ### Spotting inconsistencies
  * ### Outliers
  * ### Missing data
* ### Process and best practices

---

class: middle

# Step 1: from messy data to tidy data

Tidy data is a standard way of mapping a dataset to its structure:
* Each variable/feature forms a column
* Each observation forms a row
* Each type of observational unit forms a table

![](img/from-messy-to-tidy.png)

.footnote[.right[http://vita.had.co.nz/papers/tidy-data.pdf]]
---

class: middle

# Step 2: from tidy data to clean data

### What is data quality?

[Wikipedia](https://en.wikipedia.org/wiki/Data_quality) general definition:

* it's fit for its intended use
* it correctly represents the reality to which it refers

--

State of completeness, consistency, timeliness, duplication and accuracy that makes data appropriate for a specific use

  <!--  https://iso25000.com/index.php/en/iso-25000-standards/iso-25012
  * ISO 25012 criteria: Accuracy, Completeness, Consistency, Credibility, Currentness, Accessibility, Compliance, Confidenciality, Efficiency, Precision, Traceability, Understandability, Availability, Portability, Recoverability
 -->

<!-- ---

class: middle

# Pre-processing

* Spotting inconsistencies

* Outliers

* Missing Data -->

---

class: middle

# Mental Health in Tech Survey (2014)

Measures attitudes towards mental health and frequency of mental health disorders in the tech workplace.

![](img/df-head.png)
Answers to 25 questions from 1259 respondents.

.footnote[.right[https://www.kaggle.com/osmi/mental-health-in-tech-survey/]]

---
class: middle

.left-column[
### Spotting inconsistencies
]

.right-column[
* Irrelevant data (columns, rows)

* Duplicates

* Type conversions

* Syntax errors (white spaces, typos)

* Non standard missing values (_N/A, Null, Not Applicable_)
]

---

class: middle

.left-column[
### Spotting inconsistencies
]

.right-column[

![](img/hist_before.png)

```python
df['Age'].describe()
```

```markdown
count          1259.000
mean       79428148.311
std      2818299442.982
min           -1726.000
25%              27.000
50%              31.000
75%              36.000
max     99999999999.000
Name: Age, dtype: float64
```

]

---

class: middle

.left-column[
### Spotting inconsistencies
]

.right-column[

```python
min_max_age = (df['Age'] < 14) | (df['Age'] > 100)
df.loc[min_max_age, 'age_clean'] = np.NaN
```
![](img/hist_after.png)

```python
df['age_clean'].describe()
```
```markdown
count   1251.000
mean      32.077
std        7.288
min       18.000
25%       27.000
50%       31.000
75%       36.000
max       72.000
Name: age_clean, dtype: float64
```
]

---

class: middle

.left-column[
### Spotting inconsistencies
]

.right-column[

![](img/barchart_before.png)

```markdown
Male                                              615
male                                              206
Female                                            121
M                                                 116
female                                             62
                                                 ... 
cis male                                            1
Neuter                                              1
Androgyne                                           1
ostensibly male, unsure what that really means      1
maile                                               1
Name: Gender, Length: 49, dtype: int64
```
]

---

class: middle

.left-column[
### Spotting inconsistencies
]

.right-column[

```python
def clean_gender(s):

    if re.findall('female|woman', s, re.IGNORECASE)
      or s.lower()[0] == 'f':
        return 'woman'
    elif re.findall('male|man', s, re.IGNORECASE)
      or s.lower()[0] == 'm':
        return 'man'
    else:
        if 'queer' in s.lower()
          or 'binary' in s.lower()
          or 'agender' in s.lower():
            return 'non-binary'
        else:
            return s
```

![](img/barchart_after.png)

]

---

class: middle

.left-column[
### Spotting inconsistencies
]

.right-column[
![](img/country.png)
]

---

class: middle

.left-column[
### Spotting inconsistencies

### Outliers
]

.right-column[

No rigorous mathematical definition

A data point that differs significantly from the other observations

Causes:
* Variability

* Errors
]

---
class: middle

.left-column[
### Spotting inconsistencies

### Outliers
]

.right-column[
.center[
### Tuckey's fences (Boxplot rule)
]

![](img/boxplot.png)

.center[`[Q1 - k (IQR), Q3 + k(IQR)]`

Traditionally `k=1.5`, or `k=3` for _extreme outliers_]

]


<!-- ---
class: middle

.left-column[
### Spotting inconsistencies

### Outliers
]

.right-column[
### Local Outlier Factor

* Locality given by the `k` nearest neighbors of a data point

* Comparison between the local densities of a data point with the local densisties of its neighbors

* Points with substantially lower density than their neighbors may be identified as outliers
] -->

---

class: middle

.left-column[
### Spotting inconsistencies

### Outliers

### Missing Data
]

.right-column[

Missing data are data points for which the type is known but not the values, e.g. an unanswered questions in a survey

<!-- Mechanisms for missing data: -->

* Missing Completely At Random (`MCAR`):
<br>.example[some people don't fill in surveys on mental health]
<!-- the reasons for a value being missing are independent of any observable or non observable variable -->

* Missing At Random (`MAR`): 
<!-- missingness is not random but can be fully explained by some variable for which the information available is complete -->
<br>.example[_men are less likely to fill in surveys on mental health for no other reason then being men_]

* Missing Not At Random (`MNAR`): 
<!-- missingness is due to unknown factors -->
<br>.example[_men are less likely to fill in surveys on mental health because of mental health somehow_]
]

---

class: middle

.left-column[
### Spotting inconsistencies

### Outliers

### Missing Data
]

.right-column[
### Deletion

* Large number of missing values in a column values are missing
* Few missing values and **not** at random

### Imputation

* **Numerical** imputation: replace all missing observations with a single value, usually the mean
* **Random selection** from the distribution
* **Hot deck** imputation: missing values are imputed from similar records in the dataset
* **KNN** imputation: determine the the nearest neighbors and use some aggregate of their values to impute the missing one

]

---
class: middle

.left-column[
### Spotting inconsistencies

### Outliers

### Missing Data
]

.right-column[
```python
df['self_employed'].isnull().sum()
```
```markdown
15
```
.center[
![](img/missing_before.png)]

```python
np.random.seed(7)
np.random.choice(
    self_employed['answer'],
    size=df['self_employed'].isnull().sum(),
    replace=True,
    p=self_employed['p']
)
```
.center[
![](img/missing_after.png)]

]
---

class: middle

.left-column[
### Spotting inconsistencies

### Outliers

### Missing Data
]

.right-column[
```python
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import OneHotEncoder, Normalizer
```
```python
features = ['gender_clean', 'state_imputed', 'se_imputed',
            'no_employees']
target = ['age_clean']

enc = OneHotEncoder(handle_unknown='ignore', sparse=False)

norm = Normalizer()

pca = PCA(n_components=5)

knn = KNeighborsRegressor(n_neighbors=5)

model = Pipeline(
    [('encoder', enc),
     ('normalizare', norm),
     ('pca', pca),
     ('knn', knn)]
  )

model.fit(
  df[df['age_clean'].notnull()][features],
  df[df['age_clean'].notnull()][target])

df.loc[df['age_clean'].isnull(), 'age_clean'] = 
  model.predict(df[df['age_clean'].isnull()][features])
```
]
---

class: middle

.left-column[
### Spotting inconsistencies

### Outliers

### Missing Data
]

.right-column[

![](img/hist_pre_imputation.png)

![](img/hist_post_imputation.png)
]
---

class: middle

# Process and best practices

**Inspection**<br>
Detect unexpected, incorrect, and inconsistent data (Data profiling and Visualization)

**Cleaning**<br>
Remove, Correct or Impute incorrect data

**Verifying**<br>
Is the data correct now? Re-inspect and test to verify no assumptions are violated

**Reporting**<br>
Keep a log of the inconsistencies and errors found and how they were addressed

Make sure every step is documented and reproducible

Avoid manual edits!

---

class: middle

# Process and best practices

![](img/preprocessing-flow.png)

---

class: middle

# Summary

* This is not an exhaustive guide to pre-processing

* Many decision steps and possible failure points

* Gain better understanding of the data and the problem at hand

* It's important that models are trained on data that is reliable, consistent and unbiased

---

class: center, middle

## slides at https://github.com/sereprz/from-hot-mess-to-information
## notebook at https://github.com/sereprz/mental-health-in-tech

---

<!-- class: bottom, right -->
class: center, middle

# Thank you!
### [@sereprz](https://twitter.com/sereprz)

    </textarea>
    <script src="js/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create({
        // .. or by using a format function
        slideNumberFormat: function (current, total) {
        return '';
        },
        countIncrementalSlides: false,
        highlightStyle: 'tomorrow-night-blue'
      });
    </script>
  </body>
</html>